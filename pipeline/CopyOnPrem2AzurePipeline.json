{
	"name": "CopyOnPrem2AzurePipeline",
	"properties": {
		"description": "This pipeline copies timesliced CSV files from on-premises C:\\Data to Azure Blob Storage as a continuous job",
		"activities": [
			{
				"name": "Copy Data1",
				"type": "Copy",
				"dependsOn": [],
				"policy": {
					"timeout": "7.00:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"source": {
						"type": "FileSystemSource",
						"recursive": true
					},
					"sink": {
						"type": "BlobSink",
						"copyBehavior": "MergeFiles"
					},
					"enableStaging": false,
					"parallelCopies": 10,
					"enableSkipIncompatibleRow": true
				},
				"inputs": [
					{
						"referenceName": "SourceDataset_cp0",
						"type": "DatasetReference"
					}
				],
				"outputs": [
					{
						"referenceName": "BlobStorageOutput1",
						"type": "DatasetReference"
					}
				]
			},
			{
				"name": "BatchScope",
				"type": "DatabricksNotebook",
				"dependsOn": [
					{
						"activity": "Copy Data1",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"timeout": "7.00:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"notebookPath": "/BigDataVis/04 Deploy for Batch Scoring"
				},
				"linkedServiceName": {
					"referenceName": "AzureDatabricks",
					"type": "LinkedServiceReference"
				}
			}
		],
		"annotations": []
	},
	"type": "Microsoft.DataFactory/factories/pipelines"
}